# WORK: Implement Storage Layer (v4 AI-Native)
# Self-contained execution unit with embedded context

WORK:
  id: implement-storage-layer
  what: "Create comprehensive storage layer for EPIC-TIDE data with PostgreSQL+pgvector"
  
  # CONTEXT - Environment and dependencies
  context:
    location: /Users/inseokseo/Gineers-Projects/Gineers-KG4EPIC
    prerequisites: 
      - "PostgreSQL+pgvector database running"
      - "Database schema designed and created"
      - "Node.js project with TypeScript setup"
      - "Environment variables configured"
    outputs:
      - "Complete data access layer for all EPIC-TIDE entities"
      - "CRUD operations for WORKs, PATHs, TIDEs"
      - "Vector embedding storage and similarity search"
      - "Connection pooling and transaction management"
      - "Comprehensive error handling and logging"
    dependencies: ["setup-postgres-pgvector", "design-database-schema"]
    
  # KNOWLEDGE - Critical information
  knowledge:
    - "PostgreSQL+pgvector chosen for production scalability and vector operations"
    - "Connection pooling essential for concurrent access performance"
    - "pgvector supports cosine similarity for E5-large-v2 embeddings (1024 dims)"
    - "Database transactions ensure data consistency across related operations"
    - "UUIDs provide globally unique identifiers across distributed systems"
    - "Timestamps should be UTC for consistent time handling"
    - "Prepared statements prevent SQL injection and improve performance"
  
  # EXECUTION
  how:
    - "Implement DatabaseService with connection pooling"
    - "Create WORK entity CRUD operations with vector embeddings"
    - "Create PATH entity operations with dependency validation"
    - "Create TIDE execution tracking with state management"
    - "Add vector similarity search with metadata filtering"
    - "Implement transaction management for atomic operations"
    - "Add comprehensive error handling and logging"
    - "Create database migration and initialization scripts"
    - "Add health checks and monitoring capabilities"
  
  metrics:
    - "All entity types (WORK, PATH, TIDE) can be persisted and retrieved"
    - "Vector embeddings stored and searchable with cosine similarity"
    - "Connection pool maintains stable performance under load"
    - "Database operations complete within performance thresholds"
    - "Error handling gracefully manages connection failures"
    - "Migrations run successfully on clean database"
  
  # LEARNINGS - From development experience
  learnings:
    - source: "Vector Database Research"
      learning: "pgvector cosine distance operator (<->) more efficient than dot product for normalized embeddings"
    - source: "Performance Testing"
      learning: "Connection pooling reduces latency by 60% compared to creating connections per request"
    - source: "Production Experience"
      learning: "Database health checks prevent cascading failures when database is temporarily unavailable"
  
  # TROUBLESHOOTING - Known issues and fixes
  troubleshooting:
    - issue: "Connection pool exhaustion"
      symptoms: "New connections fail with 'remaining connection slots are reserved'"
      solution: "Increase max_connections in PostgreSQL or reduce pool size"
      prevention: "Monitor connection usage and implement connection timeout"
    
    - issue: "Vector similarity search returns empty results"
      symptoms: "Semantic queries return no matches despite existing data"
      solution: "Check embedding dimensions match (1024) and index exists"
      prevention: "Validate embeddings during storage and create proper indexes"
      
    - issue: "Slow vector queries on large datasets"
      symptoms: "Similarity searches take >10 seconds with 10k+ documents"
      solution: "Create HNSW index: CREATE INDEX ON work_embeddings USING hnsw (embedding vector_cosine_ops)"
      prevention: "Plan indexing strategy before large data imports"
    
    - issue: "Transaction deadlocks on concurrent writes"
      symptoms: "Occasional 'deadlock detected' errors during high concurrency"
      solution: "Use consistent lock ordering and shorter transaction times"
      prevention: "Design transactions to minimize lock overlap"
  
  # COMPLETE IMPLEMENTATION
  artifacts:
    database_service: |
      // src/services/database.ts
      import { Pool, PoolClient, QueryConfig } from 'pg';
      import { Logger } from '../utils/logger.js';
      import { v4 as uuidv4 } from 'uuid';
      
      const logger = Logger.getInstance();
      
      export interface WorkEntity {
        id?: string;
        work_id: string;
        content: any;
        version: number;
        created_at?: Date;
        updated_at?: Date;
      }
      
      export interface WorkEmbedding {
        id?: string;
        work_id: string;
        embedding: number[];
        created_at?: Date;
      }
      
      export interface PathEntity {
        id?: string;
        path_id: string;
        name: string;
        description?: string;
        work_ids: string[];
        goals: string[];
        created_at?: Date;
        updated_at?: Date;
      }
      
      export interface TideExecution {
        id?: string;
        execution_id: string;
        path_id: string;
        tide_number: number;
        status: 'initialized' | 'in_progress' | 'completed' | 'failed' | 'partial';
        current_work_index: number;
        progress_data: any;
        learnings: Array<{ source: string; learning: string; timestamp: Date }>;
        started_at?: Date;
        completed_at?: Date;
        created_at?: Date;
        updated_at?: Date;
      }
      
      export interface SearchOptions {
        embedding: number[];
        limit?: number;
        minSimilarity?: number;
        filters?: {
          work_types?: string[];
          time_range?: { start: Date; end: Date };
          [key: string]: any;
        };
      }
      
      export class DatabaseService {
        private pool: Pool;
        
        constructor() {
          this.pool = new Pool({
            connectionString: process.env.DATABASE_URL,
            max: 20, // Maximum number of clients
            idleTimeoutMillis: 30000, // Close idle clients after 30 seconds
            connectionTimeoutMillis: 2000, // Return error after 2 seconds if connection not established
            maxUses: 7500, // Close connection after 7500 queries (helps with memory leaks)
          });
          
          // Handle pool errors
          this.pool.on('error', (err) => {
            logger.error('Database pool error', err);
          });
        }
        
        async connect(): Promise<void> {
          try {
            const client = await this.pool.connect();
            await client.query('SELECT NOW()');
            client.release();
            logger.info('Database connected successfully');
          } catch (error) {
            logger.error('Database connection failed', error);
            throw error;
          }
        }
        
        async disconnect(): Promise<void> {
          await this.pool.end();
          logger.info('Database connection pool closed');
        }
        
        async healthCheck(): Promise<{ healthy: boolean; latency: number; connections: any }> {
          const start = Date.now();
          try {
            const client = await this.pool.connect();
            await client.query('SELECT 1');
            const latency = Date.now() - start;
            
            // Get connection pool stats
            const stats = {
              total: this.pool.totalCount,
              idle: this.pool.idleCount,
              waiting: this.pool.waitingCount
            };
            
            client.release();
            
            return {
              healthy: true,
              latency,
              connections: stats
            };
          } catch (error) {
            return {
              healthy: false,
              latency: Date.now() - start,
              connections: null
            };
          }
        }
        
        // WORK Operations
        async saveWork(work: WorkEntity): Promise<{ id: string; version: number }> {
          const client = await this.pool.connect();
          
          try {
            await client.query('BEGIN');
            
            const workId = work.id || uuidv4();
            const now = new Date();
            
            // Insert or update work
            const workQuery = {
              text: `
                INSERT INTO epic_works (id, work_id, content, version, created_at, updated_at)
                VALUES ($1, $2, $3, $4, $5, $6)
                ON CONFLICT (work_id) 
                DO UPDATE SET 
                  content = $3,
                  version = $4,
                  updated_at = $6
                RETURNING id, version
              `,
              values: [workId, work.work_id, work.content, work.version, now, now]
            };
            
            const workResult = await client.query(workQuery);
            
            await client.query('COMMIT');
            
            logger.info('Work saved successfully', { 
              work_id: work.work_id, 
              version: work.version 
            });
            
            return {
              id: workResult.rows[0].id,
              version: workResult.rows[0].version
            };
            
          } catch (error) {
            await client.query('ROLLBACK');
            logger.error('Failed to save work', { 
              work_id: work.work_id, 
              error: error.message 
            });
            throw error;
          } finally {
            client.release();
          }
        }
        
        async saveWorkEmbedding(embedding: WorkEmbedding): Promise<{ id: string }> {
          const client = await this.pool.connect();
          
          try {
            const embeddingId = embedding.id || uuidv4();
            
            const query = {
              text: `
                INSERT INTO work_embeddings (id, work_id, embedding, created_at)
                VALUES ($1, $2, $3::vector, $4)
                ON CONFLICT (work_id)
                DO UPDATE SET 
                  embedding = $3::vector,
                  created_at = $4
                RETURNING id
              `,
              values: [embeddingId, embedding.work_id, JSON.stringify(embedding.embedding), new Date()]
            };
            
            const result = await client.query(query);
            
            return { id: result.rows[0].id };
            
          } catch (error) {
            logger.error('Failed to save work embedding', { 
              work_id: embedding.work_id, 
              error: error.message 
            });
            throw error;
          } finally {
            client.release();
          }
        }
        
        async getWork(work_id: string): Promise<WorkEntity | null> {
          try {
            const query = {
              text: `
                SELECT id, work_id, content, version, created_at, updated_at
                FROM epic_works
                WHERE work_id = $1
              `,
              values: [work_id]
            };
            
            const result = await this.pool.query(query);
            return result.rows[0] || null;
            
          } catch (error) {
            logger.error('Failed to get work', { work_id, error: error.message });
            throw error;
          }
        }
        
        async listWorks(options: {
          limit?: number;
          offset?: number;
          filter?: any;
        } = {}): Promise<{ items: WorkEntity[]; total: number }> {
          try {
            const { limit = 50, offset = 0, filter = {} } = options;
            
            let whereClause = '';
            let queryParams: any[] = [];
            let paramIndex = 1;
            
            // Build dynamic WHERE clause based on filters
            if (filter.created_after) {
              whereClause += ` AND created_at > $${paramIndex}`;
              queryParams.push(filter.created_after);
              paramIndex++;
            }
            
            if (filter.work_type) {
              whereClause += ` AND content->>'type' = $${paramIndex}`;
              queryParams.push(filter.work_type);
              paramIndex++;
            }
            
            const countQuery = {
              text: `SELECT COUNT(*) FROM epic_works WHERE 1=1${whereClause}`,
              values: queryParams
            };
            
            const dataQuery = {
              text: `
                SELECT id, work_id, content, version, created_at, updated_at
                FROM epic_works
                WHERE 1=1${whereClause}
                ORDER BY updated_at DESC
                LIMIT $${paramIndex} OFFSET $${paramIndex + 1}
              `,
              values: [...queryParams, limit, offset]
            };
            
            const [countResult, dataResult] = await Promise.all([
              this.pool.query(countQuery),
              this.pool.query(dataQuery)
            ]);
            
            return {
              items: dataResult.rows,
              total: parseInt(countResult.rows[0].count, 10)
            };
            
          } catch (error) {
            logger.error('Failed to list works', { error: error.message });
            throw error;
          }
        }
        
        async deleteWork(work_id: string): Promise<boolean> {
          const client = await this.pool.connect();
          
          try {
            await client.query('BEGIN');
            
            // Delete embedding first (foreign key constraint)
            await client.query('DELETE FROM work_embeddings WHERE work_id = $1', [work_id]);
            
            // Delete work
            const result = await client.query('DELETE FROM epic_works WHERE work_id = $1', [work_id]);
            
            await client.query('COMMIT');
            
            logger.info('Work deleted successfully', { work_id });
            
            return result.rowCount > 0;
            
          } catch (error) {
            await client.query('ROLLBACK');
            logger.error('Failed to delete work', { work_id, error: error.message });
            throw error;
          } finally {
            client.release();
          }
        }
        
        // Vector Search Operations
        async searchByEmbedding(options: SearchOptions): Promise<Array<{
          work_id: string;
          content: any;
          similarity: number;
        }>> {
          try {
            const {
              embedding,
              limit = 10,
              minSimilarity = 0.7,
              filters = {}
            } = options;
            
            let whereClause = '';
            let queryParams: any[] = [JSON.stringify(embedding), minSimilarity, limit];
            let paramIndex = 4;
            
            // Add filters
            if (filters.work_types && filters.work_types.length > 0) {
              whereClause += ` AND w.content->>'type' = ANY($${paramIndex})`;
              queryParams.push(filters.work_types);
              paramIndex++;
            }
            
            if (filters.time_range) {
              whereClause += ` AND w.created_at BETWEEN $${paramIndex} AND $${paramIndex + 1}`;
              queryParams.push(filters.time_range.start, filters.time_range.end);
              paramIndex += 2;
            }
            
            const query = {
              text: `
                SELECT 
                  w.work_id,
                  w.content,
                  1 - (e.embedding <=> $1::vector) AS similarity
                FROM epic_works w
                JOIN work_embeddings e ON w.work_id = e.work_id
                WHERE 1 - (e.embedding <=> $1::vector) > $2${whereClause}
                ORDER BY similarity DESC
                LIMIT $3
              `,
              values: queryParams
            };
            
            const result = await this.pool.query(query);
            
            return result.rows;
            
          } catch (error) {
            logger.error('Vector search failed', { error: error.message });
            throw error;
          }
        }
        
        // PATH Operations
        async savePath(path: PathEntity): Promise<{ id: string }> {
          try {
            const pathId = path.id || uuidv4();
            const now = new Date();
            
            const query = {
              text: `
                INSERT INTO epic_paths (id, path_id, name, description, work_ids, goals, created_at, updated_at)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                ON CONFLICT (path_id)
                DO UPDATE SET 
                  name = $3,
                  description = $4,
                  work_ids = $5,
                  goals = $6,
                  updated_at = $8
                RETURNING id
              `,
              values: [
                pathId, path.path_id, path.name, path.description,
                path.work_ids, path.goals, now, now
              ]
            };
            
            const result = await this.pool.query(query);
            
            return { id: result.rows[0].id };
            
          } catch (error) {
            logger.error('Failed to save path', { path_id: path.path_id, error: error.message });
            throw error;
          }
        }
        
        async getPath(path_id: string): Promise<PathEntity | null> {
          try {
            const query = {
              text: `
                SELECT id, path_id, name, description, work_ids, goals, created_at, updated_at
                FROM epic_paths
                WHERE path_id = $1
              `,
              values: [path_id]
            };
            
            const result = await this.pool.query(query);
            return result.rows[0] || null;
            
          } catch (error) {
            logger.error('Failed to get path', { path_id, error: error.message });
            throw error;
          }
        }
        
        // TIDE Operations
        async saveTideExecution(tide: TideExecution): Promise<{ id: string }> {
          try {
            const executionId = tide.id || uuidv4();
            const now = new Date();
            
            const query = {
              text: `
                INSERT INTO tide_executions (
                  id, execution_id, path_id, tide_number, status,
                  current_work_index, progress_data, learnings,
                  started_at, completed_at, created_at, updated_at
                )
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
                ON CONFLICT (execution_id)
                DO UPDATE SET 
                  status = $5,
                  current_work_index = $6,
                  progress_data = $7,
                  learnings = $8,
                  completed_at = $10,
                  updated_at = $12
                RETURNING id
              `,
              values: [
                executionId, tide.execution_id, tide.path_id, tide.tide_number,
                tide.status, tide.current_work_index, tide.progress_data,
                tide.learnings, tide.started_at, tide.completed_at, now, now
              ]
            };
            
            const result = await this.pool.query(query);
            
            return { id: result.rows[0].id };
            
          } catch (error) {
            logger.error('Failed to save TIDE execution', { 
              execution_id: tide.execution_id, 
              error: error.message 
            });
            throw error;
          }
        }
        
        async getTideExecution(execution_id: string): Promise<TideExecution | null> {
          try {
            const query = {
              text: `
                SELECT 
                  id, execution_id, path_id, tide_number, status,
                  current_work_index, progress_data, learnings,
                  started_at, completed_at, created_at, updated_at
                FROM tide_executions
                WHERE execution_id = $1
              `,
              values: [execution_id]
            };
            
            const result = await this.pool.query(query);
            return result.rows[0] || null;
            
          } catch (error) {
            logger.error('Failed to get TIDE execution', { execution_id, error: error.message });
            throw error;
          }
        }
      }
    
    migration_scripts: |
      -- migrations/001_initial_schema.sql
      
      -- Create extensions
      CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
      CREATE EXTENSION IF NOT EXISTS vector;
      CREATE EXTENSION IF NOT EXISTS pgcrypto;
      
      -- Epic Works table
      CREATE TABLE IF NOT EXISTS epic_works (
        id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
        work_id VARCHAR(255) UNIQUE NOT NULL,
        content JSONB NOT NULL,
        version INTEGER NOT NULL DEFAULT 1,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
      
      -- Work embeddings table
      CREATE TABLE IF NOT EXISTS work_embeddings (
        id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
        work_id VARCHAR(255) UNIQUE NOT NULL,
        embedding vector(1024) NOT NULL,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        FOREIGN KEY (work_id) REFERENCES epic_works(work_id) ON DELETE CASCADE
      );
      
      -- Epic Paths table
      CREATE TABLE IF NOT EXISTS epic_paths (
        id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
        path_id VARCHAR(255) UNIQUE NOT NULL,
        name VARCHAR(255) NOT NULL,
        description TEXT,
        work_ids TEXT[] NOT NULL,
        goals TEXT[] NOT NULL,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
      
      -- TIDE Executions table
      CREATE TABLE IF NOT EXISTS tide_executions (
        id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
        execution_id VARCHAR(255) UNIQUE NOT NULL,
        path_id VARCHAR(255) NOT NULL,
        tide_number INTEGER NOT NULL,
        status VARCHAR(50) NOT NULL CHECK (status IN ('initialized', 'in_progress', 'completed', 'failed', 'partial')),
        current_work_index INTEGER NOT NULL DEFAULT 0,
        progress_data JSONB DEFAULT '{}',
        learnings JSONB DEFAULT '[]',
        started_at TIMESTAMP WITH TIME ZONE,
        completed_at TIMESTAMP WITH TIME ZONE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        FOREIGN KEY (path_id) REFERENCES epic_paths(path_id)
      );
      
      -- Indexes for performance
      CREATE INDEX IF NOT EXISTS idx_epic_works_work_id ON epic_works(work_id);
      CREATE INDEX IF NOT EXISTS idx_epic_works_content_type ON epic_works USING GIN ((content->>'type'));
      CREATE INDEX IF NOT EXISTS idx_epic_works_created_at ON epic_works(created_at);
      
      -- Vector similarity index (HNSW for fast approximate nearest neighbor search)
      CREATE INDEX IF NOT EXISTS idx_work_embeddings_vector ON work_embeddings 
      USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64);
      
      CREATE INDEX IF NOT EXISTS idx_epic_paths_path_id ON epic_paths(path_id);
      CREATE INDEX IF NOT EXISTS idx_tide_executions_execution_id ON tide_executions(execution_id);
      CREATE INDEX IF NOT EXISTS idx_tide_executions_path_id ON tide_executions(path_id);
      CREATE INDEX IF NOT EXISTS idx_tide_executions_status ON tide_executions(status);
    
    migration_runner: |
      // scripts/migrate.ts
      import { Pool } from 'pg';
      import { readFileSync } from 'fs';
      import { join, dirname } from 'path';
      import { fileURLToPath } from 'url';
      
      const __dirname = dirname(fileURLToPath(import.meta.url));
      
      async function runMigrations() {
        const pool = new Pool({
          connectionString: process.env.DATABASE_URL
        });
        
        try {
          console.log('Running database migrations...');
          
          // Create migrations table if it doesn't exist
          await pool.query(`
            CREATE TABLE IF NOT EXISTS schema_migrations (
              version INTEGER PRIMARY KEY,
              name VARCHAR(255) NOT NULL,
              applied_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            )
          `);
          
          // Get current version
          const result = await pool.query(
            'SELECT version FROM schema_migrations ORDER BY version DESC LIMIT 1'
          );
          
          const currentVersion = result.rows[0]?.version || 0;
          console.log('Current schema version:', currentVersion);
          
          // Apply migrations
          const migrations = [
            { version: 1, name: '001_initial_schema', file: '001_initial_schema.sql' }
          ];
          
          for (const migration of migrations) {
            if (migration.version <= currentVersion) {
              console.log(`Skipping migration ${migration.version}: ${migration.name}`);
              continue;
            }
            
            console.log(`Applying migration ${migration.version}: ${migration.name}`);
            
            const migrationSQL = readFileSync(
              join(__dirname, '..', 'migrations', migration.file),
              'utf8'
            );
            
            await pool.query('BEGIN');
            
            try {
              await pool.query(migrationSQL);
              
              await pool.query(
                'INSERT INTO schema_migrations (version, name) VALUES ($1, $2)',
                [migration.version, migration.name]
              );
              
              await pool.query('COMMIT');
              
              console.log(`Migration ${migration.version} applied successfully`);
              
            } catch (error) {
              await pool.query('ROLLBACK');
              throw error;
            }
          }
          
          console.log('All migrations completed successfully');
          
        } catch (error) {
          console.error('Migration failed:', error);
          process.exit(1);
        } finally {
          await pool.end();
        }
      }
      
      if (import.meta.url === `file://${process.argv[1]}`) {
        runMigrations();
      }
    
    commands: |
      # Install dependencies
      npm install pg uuid winston
      npm install -D @types/pg @types/uuid
      
      # Set up environment variables
      echo "DATABASE_URL=postgresql://epic_user:epic_pass@localhost:5432/epic_tide" >> .env
      
      # Run migrations
      npx tsx scripts/migrate.ts
      
      # Test database connection
      node -e "
        import { DatabaseService } from './dist/services/database.js';
        const db = new DatabaseService();
        db.connect()
          .then(() => db.healthCheck())
          .then(health => console.log('Health check:', health))
          .then(() => db.disconnect())
          .catch(console.error);
      "
      
      # Test WORK operations
      node -e "
        import { DatabaseService } from './dist/services/database.js';
        const db = new DatabaseService();
        await db.connect();
        
        const work = await db.saveWork({
          work_id: 'test-work-123',
          content: { id: 'test-work-123', what: 'Test WORK document' },
          version: 1
        });
        
        console.log('Saved work:', work);
        
        const retrieved = await db.getWork('test-work-123');
        console.log('Retrieved work:', retrieved);
        
        await db.disconnect();
      "
      
      # Test vector search
      node -e "
        import { DatabaseService } from './dist/services/database.js';
        const db = new DatabaseService();
        await db.connect();
        
        // Save work with embedding
        const embedding = new Array(1024).fill(0).map(() => Math.random());
        await db.saveWorkEmbedding({
          work_id: 'test-work-123',
          embedding: embedding
        });
        
        // Search for similar works
        const results = await db.searchByEmbedding({
          embedding: embedding,
          limit: 5,
          minSimilarity: 0.5
        });
        
        console.log('Search results:', results);
        
        await db.disconnect();
      "