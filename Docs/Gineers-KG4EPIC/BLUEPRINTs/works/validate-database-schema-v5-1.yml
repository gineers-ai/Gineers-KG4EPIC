# WORK: Validate and Deploy v5.1 Database Schema
# Ensures database has complete v5.1 schema with all required tables

WORK:
  id: validate-database-schema-v5-1
  what: "Deploy and validate complete v5.1 database schema"
  why: "TIDE_1 left database with partial schema - need full v5.1 compliance"
  how: "Drop existing schema, deploy v5.1 tables, run validation queries"
  version: 1.0
  
  # CONTEXT - Everything needed to understand this WORK
  context:
    background: |
      TIDE_1 created partial database schema but missing critical v5.1 tables:
      - PHASES table (hierarchy root)
      - PATH_WORKS junction table (many-to-many relationship)
      - Proper foreign keys and relationships
      
    current_state: |
      - PostgreSQL container running as kg4epic-postgres
      - pgvector extension installed
      - Some tables exist but structure unknown
      - Connection works from docker network
      
    dependencies:
      - PostgreSQL with pgvector running
      - Database connection from API container
      - v5.1 schema specification
      
  # KNOWLEDGE - Critical information for execution
  knowledge:
    v5_1_schema: |
      Required tables (6 total):
      1. phases - Root of hierarchy
      2. paths - Children of phases
      3. tides - Execution attempts
      4. works - Reusable components (no foreign key to paths)
      5. path_works - Junction table for many-to-many
      6. patterns - Extracted knowledge
      
    schema_relationships: |
      PHASE → PATH (one-to-many)
      PATH → TIDE (one-to-many)
      PATH ↔ WORK (many-to-many via path_works)
      TIDE → PATTERN (one-to-many)
      
    pgvector_requirements: |
      - Vector columns must be vector(1024) for E5-large-v2
      - Indexes needed for similarity search
      - Proper casting for vector operations
      
  # ARTIFACTS - Implementation details
  artifacts:
    migration_script: |
      -- v5.1 Complete Schema Migration
      -- WARNING: This drops and recreates all tables
      
      BEGIN;
      
      -- Drop existing tables if they exist
      DROP TABLE IF EXISTS patterns CASCADE;
      DROP TABLE IF EXISTS path_works CASCADE;
      DROP TABLE IF EXISTS tides CASCADE;
      DROP TABLE IF EXISTS works CASCADE;
      DROP TABLE IF EXISTS paths CASCADE;
      DROP TABLE IF EXISTS phases CASCADE;
      
      -- Ensure pgvector extension
      CREATE EXTENSION IF NOT EXISTS vector;
      CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
      
      -- 1. PHASES table (root of hierarchy)
      CREATE TABLE phases (
          id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
          phase_id VARCHAR(100) UNIQUE NOT NULL,
          what TEXT NOT NULL,
          version VARCHAR(10) NOT NULL DEFAULT '1.0',
          
          -- Phase-level context
          strategy JSONB,
          business JSONB,
          technical JSONB,
          
          -- Success and status
          success_criteria JSONB,
          status VARCHAR(50) DEFAULT 'planning',
          
          -- Learnings and patterns
          learnings JSONB,
          patterns_harvested JSONB,
          
          -- System fields
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );
      
      -- 2. PATHS table (children of phases)
      CREATE TABLE paths (
          id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
          path_id VARCHAR(100) UNIQUE NOT NULL,
          phase_id UUID REFERENCES phases(id) ON DELETE CASCADE,
          what TEXT NOT NULL,
          version VARCHAR(10) NOT NULL DEFAULT '1.0',
          
          -- Path context
          project JSONB,
          decisions JSONB,
          metrics JSONB,
          
          -- Status tracking
          status VARCHAR(50) DEFAULT 'pending',
          current_tide INTEGER DEFAULT 0,
          
          -- System fields
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );
      
      -- 3. WORKS table (reusable components)
      CREATE TABLE works (
          id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
          work_id VARCHAR(100) UNIQUE NOT NULL,
          what TEXT NOT NULL,
          why TEXT,
          how TEXT,
          version VARCHAR(10) NOT NULL DEFAULT '1.0',
          
          -- v4 template fields
          context JSONB,
          knowledge JSONB,
          artifacts JSONB,
          troubleshooting JSONB,
          learnings JSONB,
          
          -- Semantic fields with vectors
          what_embedding vector(1024),
          why_embedding vector(1024),
          knowledge_embedding vector(1024),
          
          -- System fields
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );
      
      -- 4. PATH_WORKS junction table (many-to-many)
      CREATE TABLE path_works (
          path_id UUID REFERENCES paths(id) ON DELETE CASCADE,
          work_id UUID REFERENCES works(id) ON DELETE CASCADE,
          sequence INTEGER NOT NULL,
          purpose TEXT,
          PRIMARY KEY (path_id, work_id)
      );
      
      -- 5. TIDES table (execution attempts)
      CREATE TABLE tides (
          id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
          tide_number INTEGER NOT NULL,
          path_id UUID REFERENCES paths(id) ON DELETE CASCADE,
          
          -- Execution tracking
          started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          completed_at TIMESTAMP,
          outcome VARCHAR(50),
          
          -- Context and results
          context JSONB,
          decisions JSONB,
          execution JSONB,
          learnings JSONB,
          troubleshooting JSONB,
          
          -- Unique constraint
          UNIQUE(path_id, tide_number)
      );
      
      -- 6. PATTERNS table (extracted knowledge)
      CREATE TABLE patterns (
          id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
          pattern_name VARCHAR(200) NOT NULL,
          source_tide_id UUID REFERENCES tides(id),
          
          -- Pattern details
          pattern_type VARCHAR(50),
          problem_category VARCHAR(100),
          solution JSONB,
          evidence JSONB,
          reusability_score INTEGER CHECK (reusability_score >= 1 AND reusability_score <= 10),
          
          -- Semantic search
          pattern_embedding vector(1024),
          
          -- System fields
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );
      
      -- Create indexes for performance
      CREATE INDEX idx_phases_status ON phases(status);
      CREATE INDEX idx_paths_phase ON paths(phase_id);
      CREATE INDEX idx_paths_status ON paths(status);
      CREATE INDEX idx_works_work_id ON works(work_id);
      CREATE INDEX idx_tides_path ON tides(path_id);
      CREATE INDEX idx_tides_outcome ON tides(outcome);
      CREATE INDEX idx_patterns_type ON patterns(pattern_type);
      CREATE INDEX idx_patterns_category ON patterns(problem_category);
      
      -- Create indexes for vector similarity search
      CREATE INDEX idx_works_what_embedding ON works USING ivfflat (what_embedding vector_cosine_ops) WITH (lists = 100);
      CREATE INDEX idx_works_knowledge_embedding ON works USING ivfflat (knowledge_embedding vector_cosine_ops) WITH (lists = 100);
      CREATE INDEX idx_patterns_embedding ON patterns USING ivfflat (pattern_embedding vector_cosine_ops) WITH (lists = 100);
      
      -- Create update trigger for updated_at
      CREATE OR REPLACE FUNCTION update_updated_at_column()
      RETURNS TRIGGER AS $$
      BEGIN
          NEW.updated_at = CURRENT_TIMESTAMP;
          RETURN NEW;
      END;
      $$ language 'plpgsql';
      
      -- Apply trigger to all tables with updated_at
      CREATE TRIGGER update_phases_updated_at BEFORE UPDATE ON phases FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
      CREATE TRIGGER update_paths_updated_at BEFORE UPDATE ON paths FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
      CREATE TRIGGER update_works_updated_at BEFORE UPDATE ON works FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
      CREATE TRIGGER update_patterns_updated_at BEFORE UPDATE ON patterns FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
      
      COMMIT;
      
    validation_queries: |
      -- Validation queries to confirm schema
      
      -- 1. Check all tables exist
      SELECT table_name 
      FROM information_schema.tables 
      WHERE table_schema = 'public' 
      ORDER BY table_name;
      -- Expected: 6 tables (path_works, paths, patterns, phases, tides, works)
      
      -- 2. Check vector columns
      SELECT column_name, data_type 
      FROM information_schema.columns 
      WHERE table_name IN ('works', 'patterns') 
      AND data_type = 'USER-DEFINED';
      -- Expected: 4 vector columns
      
      -- 3. Check foreign keys
      SELECT 
        tc.table_name, 
        kcu.column_name, 
        ccu.table_name AS foreign_table,
        ccu.column_name AS foreign_column 
      FROM information_schema.table_constraints AS tc 
      JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name
      JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name
      WHERE tc.constraint_type = 'FOREIGN KEY';
      -- Expected: 6 foreign key relationships
      
      -- 4. Test insert hierarchy
      INSERT INTO phases (phase_id, what) VALUES ('test_phase', 'Test phase');
      INSERT INTO paths (path_id, phase_id, what) 
      SELECT 'test_path', id, 'Test path' FROM phases WHERE phase_id = 'test_phase';
      INSERT INTO works (work_id, what) VALUES ('test_work', 'Test work');
      
      -- 5. Test junction table
      INSERT INTO path_works (path_id, work_id, sequence)
      SELECT p.id, w.id, 1 
      FROM paths p, works w 
      WHERE p.path_id = 'test_path' AND w.work_id = 'test_work';
      
      -- 6. Clean up test data
      DELETE FROM phases WHERE phase_id = 'test_phase';
      DELETE FROM works WHERE work_id = 'test_work';
      
    node_validation: |
      // TypeScript validation from Node.js
      import { Pool } from 'pg';
      
      async function validateSchema() {
        const pool = new Pool({
          connectionString: process.env.DATABASE_URL
        });
        
        try {
          // Check table count
          const tableResult = await pool.query(`
            SELECT COUNT(*) as table_count 
            FROM information_schema.tables 
            WHERE table_schema = 'public'
          `);
          
          if (tableResult.rows[0].table_count !== '6') {
            throw new Error(`Expected 6 tables, found ${tableResult.rows[0].table_count}`);
          }
          
          // Check vector extension
          const vectorResult = await pool.query(`
            SELECT * FROM pg_extension WHERE extname = 'vector'
          `);
          
          if (vectorResult.rows.length === 0) {
            throw new Error('pgvector extension not installed');
          }
          
          console.log('✅ Database schema v5.1 validation successful');
          return true;
          
        } catch (error) {
          console.error('❌ Schema validation failed:', error);
          return false;
        } finally {
          await pool.end();
        }
      }
      
  # TROUBLESHOOTING - Known issues and solutions
  troubleshooting:
    connection_refused: |
      Problem: Cannot connect to database
      Solution: Ensure DATABASE_URL uses 'postgres' as hostname within Docker network
      
    permission_denied: |
      Problem: Cannot create tables
      Solution: Ensure user has CREATE privileges: GRANT ALL ON DATABASE kg4epic_db TO kg4epic_user;
      
    vector_type_not_found: |
      Problem: type "vector" does not exist
      Solution: CREATE EXTENSION vector; must be run first
      
    foreign_key_violation: |
      Problem: Cannot delete due to foreign keys
      Solution: Use CASCADE in DROP statements or delete in correct order
      
  # SUCCESS CRITERIA
  success_criteria:
    - All 6 tables created successfully
    - Vector columns properly configured
    - All foreign keys established
    - Validation queries return expected results
    - Node.js can connect and query
    - Test data can be inserted and deleted
    
  # EXECUTION NOTES
  execution_notes: |
    1. First check what tables currently exist
    2. Backup any existing data if needed
    3. Run migration script in transaction
    4. Execute validation queries
    5. Test from Node.js application
    6. Document final schema state
    
  # PHASE ALIGNMENT
  phase_context:
    phase_id: phase_1_free
    contribution: "Completes database foundation for EPIC-TIDE storage"
    enables: "Full CRUD operations on all EPIC-TIDE entities"